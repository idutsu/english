from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

MODEL_PATH = "/models/Llama-3-ELYZA-JP-8B"

print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
tok = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    torch_dtype=torch.float16
)

messages = [
    {"role": "system", "content": "ã‚ãªãŸã¯è‹±èªä¾‹æ–‡ã‚’å‡ºã™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
    {"role": "user", "content": "ãƒ•ãƒ¬ãƒ¼ã‚ºï¼šhave to ï¼ˆã€œã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ï¼‰ æ•°ï¼š3"}
]
prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tok(prompt, return_tensors="pt").to("cuda")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.8,
        top_p=0.95,
        pad_token_id=tok.eos_token_id
    )

print("âœ… å‡ºåŠ›çµæœ:")
print(tok.decode(outputs[0], skip_special_tokens=True))

